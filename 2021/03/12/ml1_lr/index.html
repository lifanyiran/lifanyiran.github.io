<!DOCTYPE html>
<html lang='zh-CN'>

<head>
  <meta name="generator" content="Hexo 5.4.1">
  <meta charset="utf-8">
  

  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://cdn.jsdelivr.net'>
  <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel='dns-prefetch' href='//unpkg.com'>

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  <title>机器学习I - 综述与线性回归(Linear Regression) - 屹然的空间</title>

  

  
    <meta name="description" content="机器学习是一门开发算法和统计模型的科学，计算机系统使用这些算法和模型，在没有明确指令的情况下，依靠既有模式和推理来执行任务。 计算机系统使用机器学习算法来处理大量历史数据，并识别数据模式。 这可让计算机系统根据给出的输入数据集更准确地预测结果。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习I - 综述与线性回归(Linear Regression)">
<meta property="og:url" content="http://lifanyiran.com/2021/03/12/ml1_lr/index.html">
<meta property="og:site_name" content="屹然的空间">
<meta property="og:description" content="机器学习是一门开发算法和统计模型的科学，计算机系统使用这些算法和模型，在没有明确指令的情况下，依靠既有模式和推理来执行任务。 计算机系统使用机器学习算法来处理大量历史数据，并识别数据模式。 这可让计算机系统根据给出的输入数据集更准确地预测结果。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s3.bmp.ovh/imgs/2022/06/30/f89ca6f39183051e.png">
<meta property="og:image" content="https://s3.bmp.ovh/imgs/2022/06/30/d46e9246ecc7212a.png">
<meta property="og:image" content="https://s3.bmp.ovh/imgs/2022/06/30/5540c0307e3d9e6c.png">
<meta property="og:image" content="https://s3.bmp.ovh/imgs/2022/06/30/9b54479fd9b8c94b.png">
<meta property="og:image" content="d:/Users/fyrli/AppData/Roaming/Typora/typora-user-images/image-20220127173806050.png">
<meta property="og:image" content="d:/Users/fyrli/AppData/Roaming/Typora/typora-user-images/image-20220127173919580.png">
<meta property="og:image" content="d:/Users/fyrli/AppData/Roaming/Typora/typora-user-images/image-20220127174758076.png">
<meta property="og:image" content="d:/Users/fyrli/AppData/Roaming/Typora/typora-user-images/image-20220207150829824.png">
<meta property="og:image" content="d:/Users/fyrli/AppData/Roaming/Typora/typora-user-images/image-20220208132726953.png">
<meta property="article:published_time" content="2021-03-12T09:19:00.000Z">
<meta property="article:modified_time" content="2022-06-30T05:42:33.260Z">
<meta property="article:author" content="lifanyiran">
<meta property="article:tag" content="Notes">
<meta property="article:tag" content="Data Science">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://s3.bmp.ovh/imgs/2022/06/30/f89ca6f39183051e.png">
  
  

  <!-- feed -->
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  
    <link rel="shortcut icon" href="/images/favicon.ico">
  

  
</head>

<body>
  




  <div class='l_body' id='start'>
    <aside class='l_left' layout='post'>
    


<header class="header">

<div class="logo-wrap"><a class="avatar" href="/about/"><div class="bg" style="opacity:0;background-image:url(https://cdn.jsdelivr.net/gh/cdn-x/placeholder@1.0.2/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="/images/avatar.png" onerror="javascript:this.classList.add('error');this.src='https://cdn.jsdelivr.net/gh/cdn-x/placeholder@1.0.1/image/2659360.svg';"></a><a class="title" href="/"><div class="main">屹然的空间</div><div class="sub cap">Yiran's Space</div></a></div>
<nav class="menu dis-select"><a class="nav-item active" href="/">文章</a><a class="nav-item" href="/notes/">书签</a><a class="nav-item" href="/more/">更多</a></nav></header>

<div class="widgets">

<div class="widget-wrap single" id="toc"><div class="widget-header cap dis-select"><span class="name">本文目录</span></div><div class="widget-body fs14"><div class="doc-tree active"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%85%A5%E9%97%A8%E4%B8%8E%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">1. 入门与一元线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E7%AE%80%E4%BB%8B"><span class="toc-text">1.1 简介</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E6%A8%A1%E5%9E%8B%E4%B8%8E%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-Model-and-Cost-Function"><span class="toc-text">1.2 模型与代价函数 (Model and Cost Function)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-Gradient-Descent"><span class="toc-text">1.3 梯度下降 (Gradient Descent)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-1-%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-text">1.3.1 学习率</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-2-%E5%B1%80%E9%83%A8%E6%9C%80%E4%BC%98%E4%B8%8E%E5%85%A8%E5%B1%80%E6%9C%80%E4%BC%98"><span class="toc-text">1.3.2 局部最优与全局最优</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-3-%E5%81%8F%E5%BE%AE%E5%88%86%E9%A1%B9"><span class="toc-text">1.3.3 偏微分项</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-Gradient-Descent"><span class="toc-text">1.4 线性回归梯度下降 (Gradient Descent)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-Multivariate-Linear-Regression"><span class="toc-text">2. 多元线性回归(Multivariate Linear Regression)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">2.1 多元线性回归的梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-1-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE-Feature-Scaling-%E4%B8%8E%E6%A0%87%E5%87%86%E5%8C%96-Normalization"><span class="toc-text">2.1.1 特征缩放(Feature Scaling)与标准化(Normalization)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-2-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92-Polynomial-Regression"><span class="toc-text">2.1.2 多项式回归(Polynomial Regression)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B-Normal-Equation"><span class="toc-text">2.2 多元线性回归的正规方程(Normal Equation)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%AF%B9%E6%AF%94%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="toc-text">2.3 对比梯度下降与正规方程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87"><span class="toc-text">3. 优化目标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-text">3.1 代价函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">3.2 梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98-Multiclass-Classification"><span class="toc-text">3.3 多分类问题(Multiclass Classification)</span></a></li></ol></li></ol></div></div></div>


</div>
<footer class="footer dis-select"><div class="social-wrap"><a class="social" href="https://github.com/lifanyiran" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/images/github.png"/></a><a class="social" href="https://www.linkedin.com/in/lifanyiran/" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/images/linkedin.png"/></a><a class="social" href="/images/wechatqr.jpg" rel="noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/images/wechat.png"/></a><a class="social" href="/yiran.lifan@gmail.com" rel="noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/images/email.png"/></a></div></footer>

    </aside>
    <div class='l_main'>
      

      

<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a><span class="sep"></span><a class="cap breadcrumb" href="/">文章</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/Machine-Learning/">Machine Learning</a></div><div id="post-meta">发布于&nbsp;<time datetime="2021-03-12T09:19:00.000Z">2021-03-12</time></div></div>

<article class='content md post'>
<h1 class="article-title"><span>机器学习I - 综述与线性回归(Linear Regression)</span></h1>
<p>机器学习是一门开发算法和统计模型的科学，计算机系统使用这些算法和模型，在没有明确指令的情况下，依靠既有模式和推理来执行任务。 计算机系统使用机器学习算法来处理大量历史数据，并识别数据模式。 这可让计算机系统根据给出的输入数据集更准确地预测结果。</p>
<span id="more"></span>
<h2 id="1-入门与一元线性回归"><a href="#1-入门与一元线性回归" class="headerlink" title="1. 入门与一元线性回归"></a>1. 入门与一元线性回归</h2><h3 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h3><p>机器学习模型主要有以下几种分类：</p>
<ul>
<li>Supervised Machine Learning(监督学习)</li>
<li>Unsupervised Machine Learning(无监督学习)</li>
<li>Semi-supervised Machine Learning(半监督学习)</li>
<li>Reinforcement(强化学习)</li>
</ul>
<p>主要先介绍监督学习与无监督学习</p>
<p><strong>Supervised Machine Learning(监督学习)</strong></p>
<p>在监督学习中，数据的训练集（输入数据）包括了标签，该标签可能是离散的或者连续的。例如分类模型，数据的训练集中都标注了该图片是狗或者是非狗，或者是某种狗的类型。<br><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s3.bmp.ovh/imgs/2022/06/30/f89ca6f39183051e.png" alt="二元分类模型：狗"><br><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s3.bmp.ovh/imgs/2022/06/30/d46e9246ecc7212a.png" alt="多元分类模型：狗"><br>在回归模型中，需要预测的值为一个连续值，所以需要通过一些评判标准来进行预测。例如线性回归，是将每个点到预测线的误差值得集合最小化，来获得预测线。<br><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s3.bmp.ovh/imgs/2022/06/30/5540c0307e3d9e6c.png" alt="线性回归模型：冰激凌价格vs温度"></p>
<p><strong>Unsupervised Machine Learning(无监督学习)</strong></p>
<p>在现实生活中，带有标签的数据获得成本实际很高，所以催生了无监督学习的发展。与监督学习相对应的为无监督学习，即训练集中无现有标签。<br>常见的为Culstering Example(聚类模型)，根据准则（例如K-means和KNN）来将模型分类n类。<br><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s3.bmp.ovh/imgs/2022/06/30/9b54479fd9b8c94b.png" alt="聚类模型"></p>
<h3 id="1-2-模型与代价函数-Model-and-Cost-Function"><a href="#1-2-模型与代价函数-Model-and-Cost-Function" class="headerlink" title="1.2 模型与代价函数 (Model and Cost Function)"></a>1.2 模型与代价函数 (Model and Cost Function)</h3><p><strong>1.2.1 模型</strong></p>
<p>$x^{(i)}$ 输入变量 (input variables)</p>
<p>$y^{(i)}$ 输出变量 (output variables)</p>
<p>$(x^{(i)}, y^{(i)})$ 训练样本 (training example)</p>
<p>$(x^{(i)}, y^{(i)}); i = 1, …, m$ 训练集 (traing set)</p>
<p>$X, Y$ 输入特征和输出特征的空间(space)，例如$X \in \mathbb{R}, y \in \mathbb{R}$</p>
<p>一个有监督模型的过程，正式来讲，是根据训练集去学习一个假设(hypothesis)$h:X \to Y$，这样$h(x^{(i)})$便可以用来预测实际的$y^{(i)}$。例如下图：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">A(训练集 - Training Set):::box</span><br><span class="line">--&gt; B(算法 - Learning Algorithm):::box</span><br><span class="line">--&gt; C(h):::box</span><br><span class="line">--&gt; y:::box</span><br><span class="line">x:::box --&gt; C</span><br><span class="line">classDef box fill:#ebf2ff, stroke:#000000</span><br></pre></td></tr></table></figure>
<blockquote>
<p>假设可以简单理解为是一种我们推测出的一种，事物间的关系。从数据的角度，关系即为一个函数。</p>
</blockquote>
<p>衡量函数$h$的性能，我们一般使用代价函数(Cost Function)。</p>
<p><strong>1.2.2 代价函数 (Cost Function)</strong></p>
<p>以线性回归为例，$h_\theta(x) = \theta_0 + \theta_1x$， 则线性回归的代价函数$J$则为平均预测值$h(x)$和实际值$y$之间的差，如：</p>
<script type="math/tex; mode=display">
J(\theta_0,\theta_1) = \frac{1}{2m}(\hat{y}_i - y_i)^2 = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x) - y_i)^2</script><p>其中： $\theta$  函数的参数，$m$ 训练集样本量</p>
<p>线性回归的代价函数一般也称之为MSE(均方误差 Mean Sqaure Error)，或者方差损失函数(Squared Error Function)。</p>
<blockquote>
<p>乘以$\frac{1}{2}$是为了在使用梯度下降时计算的便利性，并无实际意义</p>
</blockquote>
<h3 id="1-3-梯度下降-Gradient-Descent"><a href="#1-3-梯度下降-Gradient-Descent" class="headerlink" title="1.3 梯度下降 (Gradient Descent)"></a>1.3 梯度下降 (Gradient Descent)</h3><p>目前我们拥有了假设函数$h_\theta$，以及测量假设函数的准确性的方法$J(\theta_0,\theta_1)$，我们便需要一个方法可以将代价函数最小化，从而最大限度的提升假设函数的准确性，这个方法便是梯度下降。</p>
<p>想象我们绘制一个三维曲面图，将假设函数$h_\theta$的参数$\theta_0,\theta_1$作为图的$x, y$轴，将$J(\theta_0,\theta_1)$作为$z$轴，梯度下降的概念是，我们随机选择一个点，然后环顾一周选择下降最快的一个方向，往下走一步，之后在不断重复，直到在最低点。</p>
<p>梯度下降的详细步骤如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&重复直到收敛\ \{ \\ \ \ \ \ \  
&\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta_0,\theta_1) \ \ (for\ j = 0\ and\  j = 1) \\ 
&\}
\end{aligned}</script><blockquote>
<p>$:=$ 表示复制；$\alpha$代表学习率(Learning Rate)，也就是每次梯度下降的步长</p>
</blockquote>
<h4 id="1-3-1-学习率"><a href="#1-3-1-学习率" class="headerlink" title="1.3.1 学习率"></a>1.3.1 学习率</h4><ul>
<li>如果学习率过小，梯度下降会很慢</li>
<li>如果学习率过大，梯度下降会略过收敛点，导致结果无法收敛</li>
</ul>
<h4 id="1-3-2-局部最优与全局最优"><a href="#1-3-2-局部最优与全局最优" class="headerlink" title="1.3.2 局部最优与全局最优"></a>1.3.2 局部最优与全局最优</h4><p>梯度下降会到局部最优，如果损失函数是凹函数，则不会有问题。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="D:\Users\fyrli\AppData\Roaming\Typora\typora-user-images\image-20220127173806050.png" alt=""></p>
<h4 id="1-3-3-偏微分项"><a href="#1-3-3-偏微分项" class="headerlink" title="1.3.3 偏微分项"></a>1.3.3 偏微分项</h4><p>偏微分项$\frac{\partial}{\partial\theta_j} J(\theta_0,\theta_1)$是为了让每次的步长，越下降越小，以防止略过收敛点。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="D:\Users\fyrli\AppData\Roaming\Typora\typora-user-images\image-20220127173919580.png" alt=""></p>
<h3 id="1-4-线性回归梯度下降-Gradient-Descent"><a href="#1-4-线性回归梯度下降-Gradient-Descent" class="headerlink" title="1.4 线性回归梯度下降 (Gradient Descent)"></a>1.4 线性回归梯度下降 (Gradient Descent)</h3><p>线性回归的梯度下降过程如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&重复直到收敛\ \{ \\ \ \ \ \ \  
&\theta_0 := \theta_0 - \alpha \frac{1}{m}(h_\theta(x^{(i)}) - y^{(i)})\\ \ \ \ \ \ &\theta_1 := \theta_1 - \alpha \frac{1}{m}(h_\theta(x^{(i)}) - y^{(i)})*x^{(i)}\\ 
&\}
\end{aligned}</script><p>其中：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial}{\partial\theta_j} J(\theta) 
&= \frac{\partial}{\partial\theta_j} \frac{1}{2}(h_\theta(x)-y)^2 \\
&= 2 * \frac{1}{2}(h_\theta(x)-y)*\frac{\partial}{\partial\theta_j}(h_\theta(x)-y) \\
&= (h_\theta(x)-y) * \frac{\partial}{\partial\theta_j}(\sum_{i=0}^n \theta_ix_i - y) \\
&= (h_\theta(x) - y)x_j
\end{align}</script><p>线性回归的代价函数为凹函数，所以local minimum即为overall minimum。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="D:\Users\fyrli\AppData\Roaming\Typora\typora-user-images\image-20220127174758076.png" alt=""></p>
<h2 id="2-多元线性回归-Multivariate-Linear-Regression"><a href="#2-多元线性回归-Multivariate-Linear-Regression" class="headerlink" title="2. 多元线性回归(Multivariate Linear Regression)"></a>2. 多元线性回归(Multivariate Linear Regression)</h2><p>多元线性回归是在一元线性回归的基础上，有多个$x$</p>
<p>$x_j^{(i)}$ 输入特征$j$中，第$i$个的值</p>
<p>$m$ 训练样本量</p>
<p>$n$ 特征量</p>
<p>假设函数如下：</p>
<script type="math/tex; mode=display">
h(\theta) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n</script><p>或矩阵形式：</p>
<script type="math/tex; mode=display">
h(\theta) = 
\begin{bmatrix}
\theta_0 & \theta_1 & ... & \theta_n
\end{bmatrix}
\begin{bmatrix}
x_0 \\ x_1 \\ ... \\ x_n
\end{bmatrix}
= \theta^Tx</script><h3 id="2-1-多元线性回归的梯度下降"><a href="#2-1-多元线性回归的梯度下降" class="headerlink" title="2.1 多元线性回归的梯度下降"></a>2.1 多元线性回归的梯度下降</h3><p>与一元线性回归类似</p>
<script type="math/tex; mode=display">
\begin{aligned}
&重复直到收敛\ \{\\ 
&\theta_j := \theta_j - \alpha \frac{1}{m}(h_\theta(x^{(i)}) - y^{(i)})*x_j^{(i)}\\ 
&\}
\end{aligned}</script><h4 id="2-1-1-特征缩放-Feature-Scaling-与标准化-Normalization"><a href="#2-1-1-特征缩放-Feature-Scaling-与标准化-Normalization" class="headerlink" title="2.1.1 特征缩放(Feature Scaling)与标准化(Normalization)"></a>2.1.1 特征缩放(Feature Scaling)与标准化(Normalization)</h4><p>不同的特征数值范围是不同的，比如$x_1$是从0-10，$x_2$是从0-1000，这样进行梯度下降时会过于慢，如下图：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="D:\Users\fyrli\AppData\Roaming\Typora\typora-user-images\image-20220207150829824.png" alt=""></p>
<p>调整方法有以下两种</p>
<ul>
<li><p>特征缩放：每个特征值，除以该特征的取值范围(Range)，例如$x_1/10$ ，$x_2/1000$ </p>
</li>
<li><p>标准化：$(x_i - \mu_i)/ s_i</p>
</li>
</ul>
<h4 id="2-1-2-多项式回归-Polynomial-Regression"><a href="#2-1-2-多项式回归-Polynomial-Regression" class="headerlink" title="2.1.2 多项式回归(Polynomial Regression)"></a>2.1.2 多项式回归(Polynomial Regression)</h4><p>在某些特定的输入特征，我们需要对其加工，例如合并多个特征变为一个，比如新增$x_2 = x_1 * x_1$，这样我们的假设函数就会变为</p>
<script type="math/tex; mode=display">
h(\theta) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2</script><blockquote>
<p>经过乘方之后，$x$的取值范围会大大扩大，所以特征缩放就会变得极其重要</p>
</blockquote>
<h3 id="2-2-多元线性回归的正规方程-Normal-Equation"><a href="#2-2-多元线性回归的正规方程-Normal-Equation" class="headerlink" title="2.2 多元线性回归的正规方程(Normal Equation)"></a>2.2 多元线性回归的正规方程(Normal Equation)</h3><p>正规方程为另外一种最小化代价函数的方法。与梯度下降不同的是，正规方程不是使用多次迭代，不断寻找最小值的方法，而是可以直接计算出代价函数最小时，各$\theta$的值。推导过程如下：</p>
<p>多元线性回归的假设函数为</p>
<script type="math/tex; mode=display">
h(\theta) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n</script><p>矩阵形式则为</p>
<script type="math/tex; mode=display">
h(\theta) = 
\begin{bmatrix}
\theta_0 & \theta_1 & ... & \theta_n
\end{bmatrix}
\begin{bmatrix}
x_0 \\ x_1 \\ ... \\ x_n
\end{bmatrix}
= \theta^Tx</script><p>多元线性回归的代价函数为</p>
<script type="math/tex; mode=display">
J(\theta_{0...n}) = 
\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2</script><p>带入矩阵形式</p>
<script type="math/tex; mode=display">
\begin{align}
J(\theta) &= \frac{1}{2m}(X\theta-Y)^T(X\theta - Y) \\
J(\theta) &= \frac{1}{2m}((X\theta)^T - Y^T)(X\theta - Y) \\ 
J(\theta) &= \frac{1}{2m}((X\theta)^T(X\theta) - (X\theta)^TY -Y^T(X\theta) + Y^TY) \\
J(\theta) &= \frac{1}{2m}(X^T\theta^T(X\theta) - 2X^T\theta^TY + Y^TY) \\
\end{align}</script><p>因为我们需要取代价函数的最小值，则对$J(\theta)$取偏导</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial J(\theta)}{\partial \theta} 
&= \frac{\partial \frac{1}{2m}(X^T\theta^TX\theta - 2X^T\theta^TY + Y^TY)}{\partial \theta}  \\
\frac{\partial J(\theta)}{\partial \theta} 
&= \frac{1}{2m}*2*X^TX\theta - \frac{1}{2m}*2X^TY \\
\frac{\partial J(\theta)}{\partial \theta} 
&= \frac{1}{m}(X^TX\theta - X^TY)
\end{align}</script><p>最小化$J(\theta)$时，其的导数等于0</p>
<script type="math/tex; mode=display">
\begin{align}
0 &= \frac{1}{m}(X^TX\theta - X^TY) \\
0 &= X^TX\theta - X^TY \\
X^TX\theta &= X^TY
\end{align}</script><p>假设$X^TX$是可逆的，则</p>
<script type="math/tex; mode=display">
\theta = (X^TX)^{-1}X^TY</script><blockquote>
<p> $X^TX$有时会是不可逆的，通常原因为：</p>
<ol>
<li>有些特征是冗余的，或者说一些特征相互线性相关(Linearly Dependent) </li>
<li>样本量少于特征数量，也就是$ m&lt;n $，这样我们需要对特征进行正则(Regularization)，来删除特征，后续会介绍这部分。</li>
</ol>
</blockquote>
<h3 id="2-3-对比梯度下降与正规方程"><a href="#2-3-对比梯度下降与正规方程" class="headerlink" title="2.3 对比梯度下降与正规方程"></a>2.3 对比梯度下降与正规方程</h3><p>梯度下降：</p>
<ul>
<li>需要选择学习率$\alpha$</li>
<li>需要$k$次迭代</li>
<li>时间复杂度$O(kn^2)$</li>
<li>在特征数量$n$较大时，仍然适用</li>
</ul>
<p>正规方程：</p>
<ul>
<li>不需要选择学习率，不需要迭代</li>
<li>时间复杂度$O(n^3)$</li>
<li>在特征数量比较大时，速度较慢</li>
</ul>
<h1 id="3-分类模型-Classification-与逻辑回归-Logistic-Regression"><a href="#3-分类模型-Classification-与逻辑回归-Logistic-Regression" class="headerlink" title="3. 分类模型(Classification)与逻辑回归(Logistic Regression)"></a>3. 分类模型(Classification)与逻辑回归(Logistic Regression)</h1><p>逻辑回归的假设函数为</p>
<script type="math/tex; mode=display">
h_\theta(x) = g(\theta^Tx) = g(X\theta)</script><p>其中</p>
<script type="math/tex; mode=display">
z = \theta^Tx \\
g(z) = \frac{1}{1+e^{-z}}</script><p>$g(z)$函数将实数域映射到$[0,1]$，之间从而满足分类的要求。例如，$g(z) = 0.7 $则代表分类为1的概率为70%。</p>
<p>故</p>
<script type="math/tex; mode=display">
P(y=0|x;\theta)+P(y=1|x;\theta)=1</script><h2 id="3-优化目标"><a href="#3-优化目标" class="headerlink" title="3. 优化目标"></a>3. 优化目标</h2><h3 id="3-1-代价函数"><a href="#3-1-代价函数" class="headerlink" title="3.1 代价函数"></a>3.1 代价函数</h3><p>回想线性回归的代价函数为</p>
<script type="math/tex; mode=display">
J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i=1}^m(h_\theta(x) - y_i)^2</script><p>套用到逻辑回归中，其中</p>
<script type="math/tex; mode=display">
h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}, \ y = 0 \ or \ 1</script><p>这样我们的代价函数将不会是一个凹函数，梯度下降时，会陷入局部最小值，从而无法应用梯度下降。所以我们使用以下代价函数：</p>
<script type="math/tex; mode=display">
J(\theta) = \frac{1}{m}\sum^m_{i=1}Cost(h_\theta(x^{(i)}),y^{(i)})</script><p>其中</p>
<script type="math/tex; mode=display">
Cost(h_\theta(x),y) 
=\left\{
\begin{aligned}
-log(h_\theta(x)) \ \ \ if \ \ y&=1 \\
-log(1- h_\theta(x)) \ \ \ if \ \ y&=0 
\end{aligned}
\right.</script><p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="D:\Users\fyrli\AppData\Roaming\Typora\typora-user-images\image-20220208132726953.png" alt=""></p>
<p>因为$y = 0 \ or \ 1$，所以可以简写为 </p>
<script type="math/tex; mode=display">
\begin{aligned}

Cost(h_\theta(x),y) &= 
y(-log(h_\theta(x))) + (1-y)(-log(1-h_\theta(x))) \\ 

J(\theta) &= 
-\frac{1}{m}[\sum^m_{i=1} y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\theta(x^{(i)}))] \\

&= \frac{1}{m}(-y^Tlog(h)-(1-y)^Tlog(1-h)) \ \ \ \ \ and \ \ h=g(X\theta)

\end{aligned}</script><h3 id="3-2-梯度下降"><a href="#3-2-梯度下降" class="headerlink" title="3.2 梯度下降"></a>3.2 梯度下降</h3><p>进行梯度下降，我们需要$min_\theta J(\theta)$</p>
<script type="math/tex; mode=display">
\begin{aligned}
&重复直到收敛 \{ \\
&\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j}J(\theta) \\
&\}
\end{aligned}</script><p>其中，</p>
<script type="math/tex; mode=display">
\begin{aligned}

\frac{\partial}{\partial\theta_j}J(\theta) 
&= \frac{\partial}{\partial\theta_j}[-\frac{1}{m}[\sum^m_{i=1} y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\theta(x^{(i)}))]] \\

&= \theta_j - \frac{\alpha}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \\

&= \theta - \frac{\alpha}{m}X^T(g(X\theta)-Y)

\end{aligned}</script><blockquote>
<p>有更复杂的优化方法，例如”Conjugate gradient”, “BFGS”, 和”L-BFGS”，可以在包中供我们使用</p>
</blockquote>
<h3 id="3-3-多分类问题-Multiclass-Classification"><a href="#3-3-多分类问题-Multiclass-Classification" class="headerlink" title="3.3 多分类问题(Multiclass Classification)"></a>3.3 多分类问题(Multiclass Classification)</h3><p>我们可以建立多个假设函数，每个假设函数将多个分类拆分为其中一个分类与全部其他分类，例如：</p>
<script type="math/tex; mode=display">
y\in\{0,1,...,n\} \\
h_\theta^{(0)}=P(y=0|x;\theta) \\
h_\theta^{(1)}=P(y=1|x;\theta) \\
... \\
h_\theta^{(n)}=P(y=n|x;\theta) \\</script><p><em>reference</em><br>机器学习 by 周志华</p>


<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>许可协议</span></div><div class="body"><p>本文使用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)</a> 许可协议，转载请注明出处。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="header cap theme"><span>接下来阅读</span></section><section class="body fs14"><div class="line"></div><a id="prev" href="/2021/03/13/ml2_reg/">机器学习II - 正则化(Regularzation)<span class="note">较新</span></a></section></div>








      
<footer class="page-footer reveal fs12"><hr><div class="text"><p>All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>.</p>
<p>Created By <a href="http://lifanyiran.com/">@lifanyiran</a>，powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> and <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.6.1" title="v1.6.1">Stellar</a>.</p>
</div></footer>

      <div class='float-panel mobile-only blur' style='display:none'>
  <button type='button' class='sidebar-toggle mobile' onclick='sidebar.toggle()'>
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"></path><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"></path></svg>
  </button>
</div>

    </div>
  </div>
  <div class='scripts'>
    <script type="text/javascript">
  stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.6.1';
  stellar.config = {
    date_suffix: {
      just: '刚刚',
      min: '分钟前',
      hour: '小时前',
      day: '天前',
      month: '个月前',
    },
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js',
    sitesjs: '/js/plugins/sites.js',
    friendsjs: '/js/plugins/friends.js',
  };

  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.3.1/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@6/swiper-bundle.min.css","js":"https://unpkg.com/swiper@6/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://cdn.jsdelivr.net/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://cdn.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://cdn.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://cdn.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti/umd/heti.min.css","js":"https://unpkg.com/heti/umd/heti-addon.min.js"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->



<!-- inject -->


  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
