<!DOCTYPE html>
<html lang='zh-CN'>

<head>
  <meta name="generator" content="Hexo 5.4.1">
  <meta charset="utf-8">
  

  <meta http-equiv='x-dns-prefetch-control' content='on' />
  <link rel='dns-prefetch' href='https://cdn.jsdelivr.net'>
  <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel='dns-prefetch' href='//unpkg.com'>

  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  <title>机器学习III - 神经网络(Neural Networks) - 屹然的空间</title>

  

  
    <meta name="description" content="5. 神经网络(Neural Networks)神经网络是通过模拟大脑的构造，来构建的机器学习模型。模型是由很多个逻辑单元(Logistic Unit)来组成的。">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习III - 神经网络(Neural Networks)">
<meta property="og:url" content="http://lifanyiran.com/2021/03/14/ml3_nn/index.html">
<meta property="og:site_name" content="屹然的空间">
<meta property="og:description" content="5. 神经网络(Neural Networks)神经网络是通过模拟大脑的构造，来构建的机器学习模型。模型是由很多个逻辑单元(Logistic Unit)来组成的。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="d:/Users/fyrli/AppData/Roaming/Typora/typora-user-images/image-20220208164233500.png">
<meta property="og:image" content="d:/Users/fyrli/AppData/Roaming/Typora/typora-user-images/image-20220209135320827.png">
<meta property="og:image" content="d:/Users/fyrli/AppData/Roaming/Typora/typora-user-images/image-20220209135718142.png">
<meta property="og:image" content="d:/Users/fyrli/AppData/Roaming/Typora/typora-user-images/image-20220210100703924.png">
<meta property="article:published_time" content="2021-03-14T10:23:00.000Z">
<meta property="article:modified_time" content="2022-06-30T05:46:09.812Z">
<meta property="article:author" content="lifanyiran">
<meta property="article:tag" content="Notes">
<meta property="article:tag" content="Data Science">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="d:/Users/fyrli/AppData/Roaming/Typora/typora-user-images/image-20220208164233500.png">
  
  

  <!-- feed -->
  

  
    
<link rel="stylesheet" href="/css/main.css">

  

  
    <link rel="shortcut icon" href="/images/favicon.ico">
  

  
</head>

<body>
  




  <div class='l_body' id='start'>
    <aside class='l_left' layout='post'>
    


<header class="header">

<div class="logo-wrap"><a class="avatar" href="/about/"><div class="bg" style="opacity:0;background-image:url(https://cdn.jsdelivr.net/gh/cdn-x/placeholder@1.0.2/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="/images/avatar.png" onerror="javascript:this.classList.add('error');this.src='https://cdn.jsdelivr.net/gh/cdn-x/placeholder@1.0.1/image/2659360.svg';"></a><a class="title" href="/"><div class="main">屹然的空间</div><div class="sub cap">Yiran's Space</div></a></div>
<nav class="menu dis-select"><a class="nav-item active" href="/">文章</a><a class="nav-item" href="/notes/">书签</a><a class="nav-item" href="/more/">更多</a></nav></header>

<div class="widgets">

<div class="widget-wrap single" id="toc"><div class="widget-header cap dis-select"><span class="name">本文目录</span></div><div class="widget-body fs14"><div class="doc-tree active"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E5%9F%BA%E7%A1%80%E5%BA%94%E7%94%A8-%E9%80%BB%E8%BE%91%E5%8D%95%E5%85%83-Logistic-Unit"><span class="toc-text">5.1 基础应用 逻辑单元(Logistic Unit)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="toc-text">5.2 代价函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95-Backpropagation-Algorithm"><span class="toc-text">5.3 反向传播算法(Backpropagation Algorithm)</span></a></li></ol></div></div></div>


</div>
<footer class="footer dis-select"><div class="social-wrap"><a class="social" href="https://github.com/lifanyiran" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/images/github.png"/></a><a class="social" href="https://www.linkedin.com/in/lifanyiran/" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/images/linkedin.png"/></a><a class="social" href="/images/wechatqr.jpg" rel="noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/images/wechat.png"/></a><a class="social" href="/yiran.lifan@gmail.com" rel="noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/images/email.png"/></a></div></footer>

    </aside>
    <div class='l_main'>
      

      

<div class="bread-nav fs12"><div id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a><span class="sep"></span><a class="cap breadcrumb" href="/">文章</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/Machine-Learning/">Machine Learning</a></div><div id="post-meta">发布于&nbsp;<time datetime="2021-03-14T10:23:00.000Z">2021-03-14</time></div></div>

<article class='content md post'>
<h1 class="article-title"><span>机器学习III - 神经网络(Neural Networks)</span></h1>
<h1 id="5-神经网络-Neural-Networks"><a href="#5-神经网络-Neural-Networks" class="headerlink" title="5. 神经网络(Neural Networks)"></a>5. 神经网络(Neural Networks)</h1><p>神经网络是通过模拟大脑的构造，来构建的机器学习模型。模型是由很多个逻辑单元(Logistic Unit)来组成的。</p>
<span id="more"></span>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">  x0[x&lt;sub&gt;0&lt;/sub&gt;=1]:::trans --&gt; x4</span><br><span class="line">  x1((x&lt;sub&gt;1&lt;/sub&gt;)):::blue --&gt; x4</span><br><span class="line">  x2((x&lt;sub&gt;2&lt;/sub&gt;)):::blue --&gt; x4</span><br><span class="line">  x3((x&lt;sub&gt;3&lt;/sub&gt;)):::blue --&gt; x4</span><br><span class="line">  x4((&quot;a&quot;)):::white --&gt; y[&quot;h&lt;sub&gt;θ&lt;/sub&gt;(x)&quot;]:::trans</span><br><span class="line">classDef blue fill:#ebf2ff, stroke:#000000</span><br><span class="line">classDef trans fill:#FFFFFF, stroke:#FFFFFF</span><br><span class="line">classDef white fill:#FFFFFF, stroke:#f5c638, stroke-width:3px</span><br></pre></td></tr></table></figure>
<p>其中$x_1, x_2, x_3$为输入变量，经过神经后传递数据给单元，最后输出一个值。$h_\theta(x)$为单元的激活函数(Activateion Funtion)。例如常用的激活函数为Sigmod函数，$h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}$</p>
<blockquote>
<p>另外也会有$x_0$这种偏置单位(biased unit) = 1</p>
</blockquote>
<p>神经网络其实就是单元互相的结合</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="D:\Users\fyrli\AppData\Roaming\Typora\typora-user-images\image-20220208164233500.png" alt=""></p>
<p>其中第一层Layer1称之为输入层(Input Layer)，最后一层Layer3称之为输出层(Output Layer)，其他中间层称之为隐藏层(Hidden Layer)。其他定义：</p>
<p>$a_i^{(j)} =\ $第$j$层的第$i$个单元</p>
<p>$\Theta^{(j)}$ = 从第$j$层映射至第$j+1$层的加权矩阵</p>
<p>例如上图中的神经网络：</p>
<script type="math/tex; mode=display">
\begin{aligned}

a_1^{(2)} &= g(\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3) \\

a_2^{(2)} &= g(\Theta_{20}^{(1)}x_0+\Theta_{21}^{(1)}x_1+\Theta_{22}^{(2)}x_2+\Theta_{23}^{(1)}x_3) \\

a_3^{(2)} &= g(\Theta_{30}^{(1)}x_0+\Theta_{31}^{(1)}x_1+\Theta_{32}^{(3)}x_2+\Theta_{33}^{(1)}x_3) \\

h_\Theta(x) &= a_1^{(3)} = g(\Theta_{10}^{(2)}x_0+\Theta_{11}^{(2)}x_1+\Theta_{12}^{(2)}x_2+\Theta_{13}^{(2)}x_3)

\end{aligned}</script><p>如果神经网络在$j$层有$s_j$个单元，在$j+1$层有$s_{j+1}$个单元，则$\Theta^{(j)}$的维度为$s_{j+1} \times (s_j+1)$</p>
<p>例如上图中的神经网络，$\Theta^{(j)} = 3 \times (3+1) = 12$ </p>
<p>$z_k^{(j)} = \ $  第$j$层的第$k$个单元的所有输入的加权集合$\ = (\Theta_{k,0}^{(k-1)}x_0+\Theta_{k, 1}^{(k-1)}x_1+\Theta_{k, 2}^{(k-1)}x_2+ … +\Theta_{k, n}^{(k-1)}x_n)$ </p>
<p>矩阵形式为</p>
<script type="math/tex; mode=display">
x = 
\begin{bmatrix}
x_0 \\ x_1 \\ \vdots \\ x_n
\end{bmatrix}
, \ \ 
z^{(j)} = 
\begin{bmatrix}
z_1^{(j)} \\ z_2^{(j)} \\ \vdots \\ z_n^{(j)}
\end{bmatrix}</script><p>用$a^{(1)}$来表示$x$，则</p>
<script type="math/tex; mode=display">
\begin{aligned}
z^{(j)} &= \Theta^{(j-1)}a^{(j-1)} \\
a^{(j)} &= g(z^{(j)})
\end{aligned}</script><p>而$h_\Theta(x)$则是最后一层的$a^{(j)}$</p>
<h3 id="5-1-基础应用-逻辑单元-Logistic-Unit"><a href="#5-1-基础应用-逻辑单元-Logistic-Unit" class="headerlink" title="5.1 基础应用 逻辑单元(Logistic Unit)"></a>5.1 基础应用 逻辑单元(Logistic Unit)</h3><p>假设我们的输入层有两个单元$\{x_1,x_2\}$且$x_1, x_2 \in \{0, 1\}$代表$Ture, False$，以及偏置单位$x_0 = 1$，我们可以设计一个$\Theta$矩阵进行逻辑判断$AND, OR, XOR$等。</p>
<blockquote>
<p>$g(x)$依然是sigmod函数</p>
</blockquote>
<ul>
<li>与 AND</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
&\Theta^{(1)} = \begin{bmatrix} -30 & 20 & 20 \end{bmatrix} \\
&x_1 = 0 \quad and \quad x_2 = 0 \quad then \quad g(-30) \approx 0 \\
&x_1 = 1 \quad and \quad x_2 = 0 \quad then \quad g(-10) \approx 0 \\
&x_1 = 0 \quad and \quad x_2 = 1 \quad then \quad g(-10) \approx 0 \\
&x_1 = 1 \quad and \quad x_2 = 1 \quad then \quad g(10) \approx 1 \\
\end{aligned}</script><ul>
<li>或 OR</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
&\Theta^{(1)} = \begin{bmatrix} -10 & 20 & 20 \end{bmatrix} \\
&x_1 = 0 \quad and \quad x_2 = 0 \quad then \quad g(-10) \approx 0 \\
&x_1 = 1 \quad and \quad x_2 = 0 \quad then \quad g(10) \approx 1 \\
&x_1 = 0 \quad and \quad x_2 = 1 \quad then \quad g(10) \approx 1 \\
&x_1 = 1 \quad and \quad x_2 = 1 \quad then \quad g(40) \approx 1 \\
\end{aligned}</script><ul>
<li>或非 NOR</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
&\Theta^{(1)} = \begin{bmatrix} 10 & -20 & -20 \end{bmatrix} \\
&x_1 = 0 \quad and \quad x_2 = 0 \quad then \quad g(10) \approx 1 \\
&x_1 = 1 \quad and \quad x_2 = 0 \quad then \quad g(-10) \approx 0 \\
&x_1 = 0 \quad and \quad x_2 = 1 \quad then \quad g(-10) \approx 0 \\
&x_1 = 1 \quad and \quad x_2 = 1 \quad then \quad g(-30) \approx 0 \\
\end{aligned}</script><ul>
<li>异或非 XNOR </li>
</ul>
<p>异或非是指当且仅当$x1 = x2$时为真，此时组合以上三个逻辑单元形成一个两层的神经网络</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="D:\Users\fyrli\AppData\Roaming\Typora\typora-user-images\image-20220209135320827.png" alt=""></p>
<script type="math/tex; mode=display">
\begin{aligned}

\Theta^{(1)} &= \begin{bmatrix} -30 & 20 & 20 \\10 & -20 & -20 \end{bmatrix} \\

\\

then \quad 
a_1^{(2)} 
&= \begin{bmatrix} g(-30) & g(-10) & g(-10) & g(10) \end{bmatrix} \\
&= \begin{bmatrix} 0 & 0 & 0 & 1 \end{bmatrix} \\

a_2^{(2)} 
&= \begin{bmatrix} g(10) & g(-10) & g(-10) & g(-30) \end{bmatrix} \\
&= \begin{bmatrix} 1 & 0 & 0 & 0 \end{bmatrix} \\

\\

\Theta^{(2)} &= \begin{bmatrix} 10 & -20 & -20 \end{bmatrix} \\

\\ 

then \quad a^{(3)} 
&= \begin{bmatrix} g(-10) & g(10) & g(10) & g(-10) \end{bmatrix} 
= \begin{bmatrix} 0 & 1 & 1 & 0 \end{bmatrix} \\

\end{aligned}</script><h3 id="5-2-代价函数"><a href="#5-2-代价函数" class="headerlink" title="5.2 代价函数"></a>5.2 代价函数</h3><p>如以下的实例神经网络</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="D:\Users\fyrli\AppData\Roaming\Typora\typora-user-images\image-20220209135718142.png" alt=""></p>
<p>$L$ = 神经网络的层数，例如示例有4层</p>
<p>$s_l$ = 第$l$层单元的个数，例如示例的$s_1=3$ </p>
<blockquote>
<ul>
<li>如果是二分类(Binary Classification)问题，$s_L = 1$，因为$y = 0\ or\  1$</li>
<li>如果是多分类问题，例如$k$个分类，则$s_L = k$，因为$y\in\mathbb{R}^k,\ h_\Theta(x)\in\mathbb{R}^k$</li>
</ul>
</blockquote>
<p>逻辑回归带正则项的代价函数：</p>
<script type="math/tex; mode=display">
J(\theta) = 
-\frac{1}{m}[\sum^m_{i=1} y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\theta(x^{(i)}))]
+ \frac{\lambda}{2m}\sum^n_{j=1}\theta_j^2</script><p>对于神经网络来说，代价函数是上函数的一般化形式（逻辑回归为二分类，神经网络为多分类）</p>
<script type="math/tex; mode=display">
h_\Theta(x) \in \mathbb{R}^K, \quad(h_\Theta(x))_i = i^{th}output \\
J(\Theta) = 
-\frac{1}{m}[\sum^m_{i=1} \sum^K_{k=1} y_k^{(i)}log(h_\Theta(x^{(i)}))_k + (1-y_k^{(i)})log(1-(h_\Theta(x^{(i)}))_k)] 
+ \frac{\lambda}{2m}\sum^{L-1}_{l=1}\sum^{s_l}_{i=1}\sum^{s_{l+1}}_{j=1}(\Theta_{ji}^{(l)})^2</script><h3 id="5-3-反向传播算法-Backpropagation-Algorithm"><a href="#5-3-反向传播算法-Backpropagation-Algorithm" class="headerlink" title="5.3 反向传播算法(Backpropagation Algorithm)"></a>5.3 反向传播算法(Backpropagation Algorithm)</h3><p>反向传播算法是类似梯度下降，用于优化代价函数的算法。</p>
<p>代价函数为</p>
<script type="math/tex; mode=display">
J(\Theta) = 
-\frac{1}{m}[\sum^m_{i=1} \sum^K_{k=1} y_k^{(i)}log(h_\Theta(x^{(i)}))_k + (1-y_k^{(i)})log(1-(h_\Theta(x^{(i)}))_k)] 
+ \frac{\lambda}{2m}\sum^{L-1}_{l=1}\sum^{s_l}_{i=1}\sum^{s_{l+1}}_{j=1}(\Theta_{ji}^{(l)})^2</script><p>我们需要$min_\Theta J(\Theta)$，也就是计算$\frac{\partial}{\partial \Theta_{ij}^{(l)}J(\Theta)}$</p>
<p>例如以下例子：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="D:\Users\fyrli\AppData\Roaming\Typora\typora-user-images\image-20220210100703924.png" alt=""></p>
<p>按照我们之前的方法，计算最后的输出层为以下步骤：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a^{(1)} &= x \\
a^{(2)} &= g(z^{(2)}) \quad and \ z^{(2)} = \Theta^{(1)}a^{(1)} \\
a^{(3)} &= g(z^{(3)}) \quad and \ z^{(3)} = \Theta^{(2)}a^{(2)} \\
a^{(4)} &= h_\Theta(x) =  g(z^{(4)}) \quad and \ z^{(4)} = \Theta^{(3)}a^{(3)} \\
\end{aligned}</script><blockquote>
<p>可以理解为是正向传播(Forward Propagation)</p>
</blockquote>
<p>定义：$\delta^{(l)}_j$ = 第$l$层的第$j$个单元$a_j^{(l)}$的残差</p>
<p>对于每个单元</p>
<script type="math/tex; mode=display">
\delta^{(L)}_j = a_j^{(L)} - y_j</script><p>Layer4作为输出层，其中$\delta^{(4)}_j = a_j^{(4)} - y_j$，但是中间的隐藏层我们该如何计算残差呢？这就用到了反向传播，也就是<strong>倒推</strong>。</p>
<script type="math/tex; mode=display">
\delta^{(3)} = (\Theta^{(3)})^T\delta^{(4)}\times g'(z^{(3)}) \\
\delta^{(2)} = (\Theta^{(2)})^T\delta^{(3)}\times g'(z^{(2)})</script><blockquote>
<p>没有$\delta^{(1)}$因为第一层是输入层，没有残差</p>
</blockquote>
<p>计算好每一层的$\delta^{(l)}_j$之后，我们定义一个</p>
<script type="math/tex; mode=display">
\Delta^{(l)}_{ij} := \Delta^{(l)}_{ij} + a_j^{(l)}\delta_j^{(l+1)}</script><blockquote>
<p>所有层的单元乘以残差的总和</p>
</blockquote>
<p>之后我们定义</p>
<script type="math/tex; mode=display">
\begin{aligned}
D_{ij}^{(l)} &:= \frac{1}{m} \Delta_{ij}^{(l)} + \lambda\Theta_{ij}^{(l)} \quad if\  j \neq 0 \\
D_{ij}^{(l)} &:= \frac{1}{m} \Delta_{ij}^{(l)} \quad \quad \quad \quad \ \  if\  j = 0   \\
\end{aligned}</script><p>而</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \Theta_{ij}^{(l)}J(\Theta)} = D_{ij}^{(l)}</script>

<div class="article-footer reveal fs14"><section id="license"><div class="header"><span>许可协议</span></div><div class="body"><p>本文使用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)</a> 许可协议，转载请注明出处。</p>
</div></section></div>

</article>

<div class="related-wrap reveal" id="read-next"><section class="header cap theme"><span>接下来阅读</span></section><section class="body fs14"><a id="next" href="/2021/03/13/ml2_reg/">机器学习II - 正则化(Regularzation)<span class="note">较早</span></a><div class="line"></div><a id="prev" href="/2021/03/15/ml4_evaluate/">机器学习IV - 评估模型(Evaluating Model)<span class="note">较新</span></a></section></div>








      
<footer class="page-footer reveal fs12"><hr><div class="text"><p>All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a>.</p>
<p>Created By <a href="http://lifanyiran.com/">@lifanyiran</a>，powered by <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a> and <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.6.1" title="v1.6.1">Stellar</a>.</p>
</div></footer>

      <div class='float-panel mobile-only blur' style='display:none'>
  <button type='button' class='sidebar-toggle mobile' onclick='sidebar.toggle()'>
    <svg class="icon" style="width: 1em; height: 1em;vertical-align: middle;fill: currentColor;overflow: hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="15301"><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 2.3 26.8 24.6 47.5 51.6 47.6h416.5v4z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15302"></path><path d="M566.407 808.3c26.9-0.1 49.3-20.8 51.6-47.6-1.9-27.7-23.9-49.7-51.6-51.6h-412.6c-28.2-1.4-52.6 19.5-55.5 47.6 1.9 27.7 23.9 49.7 51.6 51.6h416.5z m309.3-249.9c26.9-0.1 49.3-20.8 51.6-47.6-2.2-26.8-24.6-47.5-51.6-47.6h-721.9c-27.7-2.8-52.5 17.4-55.3 45.1-0.1 0.8-0.1 1.7-0.2 2.5 0.9 27.2 23.6 48.5 50.7 47.6H875.707z m-103.1-245.9c26.9-0.1 49.3-20.8 51.6-47.6-0.4-28.3-23.2-51.1-51.5-51.6h-618.9c-29.5-1.1-54.3 21.9-55.5 51.4v0.2c1.4 27.8 25.2 49.2 53 47.8 0.8 0 1.7-0.1 2.5-0.2h618.8z" p-id="15303"></path></svg>
  </button>
</div>

    </div>
  </div>
  <div class='scripts'>
    <script type="text/javascript">
  stellar = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    loadCSS: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    // 从 butterfly 和 volantis 获得灵感
    loadScript: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    // https://github.com/jerryc127/hexo-theme-butterfly
    jQuery: (fn) => {
      if (typeof jQuery === 'undefined') {
        stellar.loadScript(stellar.plugins.jQuery).then(fn)
      } else {
        fn()
      }
    }
  };
  stellar.github = 'https://github.com/xaoxuu/hexo-theme-stellar/tree/1.6.1';
  stellar.config = {
    date_suffix: {
      just: '刚刚',
      min: '分钟前',
      hour: '小时前',
      day: '天前',
      month: '个月前',
    },
  };

  // required plugins (only load if needs)
  stellar.plugins = {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js',
    sitesjs: '/js/plugins/sites.js',
    friendsjs: '/js/plugins/friends.js',
  };

  // optional plugins
  if ('true' == 'true') {
    stellar.plugins.lazyload = Object.assign({"enable":true,"js":"https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.3.1/dist/lazyload.min.js","transition":"blur"});
  }
  if ('true' == 'true') {
    stellar.plugins.swiper = Object.assign({"enable":true,"css":"https://unpkg.com/swiper@6/swiper-bundle.min.css","js":"https://unpkg.com/swiper@6/swiper-bundle.min.js"});
  }
  if ('' == 'true') {
    stellar.plugins.scrollreveal = Object.assign({"enable":null,"js":"https://cdn.jsdelivr.net/npm/scrollreveal@4.0.9/dist/scrollreveal.min.js","distance":"8px","duration":500,"interval":100,"scale":1});
  }
  if ('true' == 'true') {
    stellar.plugins.preload = Object.assign({"enable":true,"service":"flying_pages","instant_page":"https://cdn.jsdelivr.net/gh/volantis-x/cdn-volantis@4.1.2/js/instant_page.js","flying_pages":"https://cdn.jsdelivr.net/gh/gijo-varghese/flying-pages@2.1.2/flying-pages.min.js"});
  }
  if ('true' == 'true') {
    stellar.plugins.fancybox = Object.assign({"enable":true,"js":"https://cdn.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.umd.js","css":"https://cdn.jsdelivr.net/npm/@fancyapps/ui@4.0/dist/fancybox.css","selector":".swiper-slide img"});
  }
  if ('false' == 'true') {
    stellar.plugins.heti = Object.assign({"enable":false,"css":"https://unpkg.com/heti/umd/heti.min.css","js":"https://unpkg.com/heti/umd/heti-addon.min.js"});
  }
</script>

<!-- required -->

  
<script src="/js/main.js" async></script>



<!-- optional -->



<!-- inject -->


  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
